# 2차시 2024.03.10

-[슬롯머신](#슬롯머신신)

-[효율적인 가치 업데이트](#효율적인_가치_업데이트)

-[$ε$-greedy 알고리즘](#$ε$-greedy_알고리즘)

## 슬롯머신

$n$개의 서로다른 슬롯머신이 있다. 슬롯머신의 레버를 당기면 랜덤한 확률로 상금을 받을 수 있는데, 각 슬롯머신이 상금을 주는 확률과 상금의 크기가 서로 다르다. 슬롯머신의 레버를 당길 수 있는 횟수가 정해져 있을 때 어떠한 방법을 사용해야 많은 상금을 받을 수 있을까?

![image](https://github.com/nkmin0/2024_RL/assets/162765658/631e4cd7-f524-4b80-a3f6-6f180034102f)

위와 같은 상황이 있다고 하자

각 슬롯머신의 레버를 한 번씩 내려 보았을 때 2번째 슬롯머신에서 주는 돈이 가장 많았기 때문에 남은 횟수를 모두 2번 슬롯머신에 사용한다면 1번 슬롯머신이나 3번 슬롯머신을 선택했을 때 보다 결과적으로 손해를 보게된다. 따라서 이러한 슬롯머신 문제에서는 에이전트는 한정된 횟수에서 지금까지 알고있던 슬롯머신에 대한 정보만을 활용해서 상금을 얻으려 할지, 혹은 다른 슬롯머신 중에서 더 좋은 슬롯머신이 있는지 를 탐색해야 할 지 고민해야한다.

- 탐색 : 아직 잘 모르는 슬롯머신을 선택함으로써 더 많은 정보를 얻고 더 좋은 상금을 줄 가능성이 있는 슬롯머신을 발견할 수 있다.
- 활용 : 현재까지 알고 있는 정보를 바탕으로 보상의 합을 극대화 하는 슬롯머신을 선택한다.

### 가치 추정

$$Q_t(i)=\frac{R_1+R_2+\cdots+R_N(t)}{N(i)}$$

$i$번째 슬롯머신에 대한 가치의 추정치는 위처럼 지금까지 $i$번째 슬롯머신을 당겨서 얻은 보상의 합을 슬롯머신을 당긴 횟수로 나누어 즉, $i$번째 슬롯머신을 당겨서 얻은 상금의 평균으로 나타낼 수 있다.

$$ a_t=argmaxQ_t(i) $$

이처럼 $i$번째 슬롯머신을 당겼을 때 얻은 상금의 평균값을 나타낸 $Q_t$에 대해서 몇번째 슬롯머신의 평균상금이 가장 큰지를 알아내어 에이전트가 알고있는 정보를 활용할 수 있다.

### 효율적인 가치 업데이트

$i$번째 슬롯머신의 평균 상금을 구하기 위해서 $i$번째 슬롯머신을 당겨서 얻은 보상의 합을 슬롯머신을 당긴 횟수로 나누어 구하였다. 하지만 평균 상금을 계속 업데이트해주기 위해서는 지금가지의 상금 기록을 보관하고 있어야 한다. 시간이 지나면서 데이터가 많아지기 때문에 메모리와 코드의 작동 시간이 증가하게 된다. 이러한 문제를 해결하기 위해서는 현재의 평균 $Q_k$와 데이터의 개수 $k$를 알고 있을 때 새로운 상금 $R_k$가 주어지면 다음 평균인 $Q_{k+1}$를 효율적으로 구해야 한다.

$$Q_{k+1}=\frac{1}{k} \sum_{j=1}^{k} R_j$$

$$ = \frac{1}{k} (R_k + \sum_{j=1}^{k-1} R_j) = \frac{1}{k} ( R_k + (k-1)Q_k ) $$

$$ = \frac{1}{k} (R_k + (k-1)Q_k + Q_k - Q_k) $$

$$ = \frac{1}{k} (R_k + kQ_k - Q-k) $$

$$ = Q_k + \frac{1}{k} (R_k - Q_k) $$

$$ Q_{k+1} &leftarrow; Q_k $$

$$ New &leftarrow; Old + Stepsize(Target - Old) $$

위처럼 평균을 업데이트할 때 모든 보상값을 저장하지 않고 이전에 가치 $Q_k$ 와 새로받은 보상 $R_k$만 가지고 다음 가치 $Q_{k+1}$을 알 수 있다.

 $Target - Old$는 오차($error$)라고도 부른다. 이러한 오차를 줄임으로써 우리가 목표하는 $Target$에 더 가까워 지도록 하는 것이다.

### 낙관주의적 탐색

초기 데이터가 존재하지 않을 때 단순히 $a_t=argmaxQ_t(i)$ 이 방법을 사용한다면 첫번째로 당긴 슬롯머신에 대해서만 $Q_t(i)$가 변화하기 때문에 제대로 된 학습이 이루어지지 않는다. 따라서 처음에 모든 슬롯머신에 대해서 몇번씩은 강제로 당기는 행동을 통해 초기 $Q_t$에 대해 값을 주어야 한다. $1~n$사이로 주어지는 기계가 여러대 있을 때 이에 대한 평균을 구할 때 $n$보다 매우 큰 $N$을 가치함수 $Q_t$에 초기값으로 주어진다면 각각의 $Q_t$에 대해 평균값으로 수렴할 때 까지 모든 기계에 대해서 작동을 강제할 수 있다.

### $ε$-greedy 알고리즘

강화학습에서는 탐색과 활용이 중요하다. 많은 정보를 얻기 위해 많은 탐색을 하면 가치가 높은 무언가를 찾을 수 도 있지만 찾지 못한다면 가치에 총합이 작아지게 된다. 반대로 활용만 하다보면 가치가 높은 다른 무언가를 놓혀 최적의 결과를 만들어내지 못할 수 있다. 이러한 문제를 해결하는 간단한 방법은 가치가 높은것을 선택하다 낮은 확률 $ε$으로 탐색을 시도하는 것이고 이러한 방법을 $\epsilon$-greedy 알고리즘이라고 부른다.

```python
for t in range(T):
  sample p ~ [0,1] # 0~1 사이의 랜덤 한 값 추출

  if p < ε:
    action = RANDOM({1,··· ,n}) # ε의 확률로 랜덤한 행동 선택(탐색)
  else :
    action = argmaxQ_t(i) # 1-ε의 확률로 가치가 가장 높은 행동 선택(활용)
  .
  .
  .
```

$\epsilon$-greedy 알고리즘은 위와 같이 어떤 작은 $ε$에 대해, 어떤 행동을 선택할 때 $ε$확률로 탐색을 시도하고 $1-ε$의 확률로 가장 가치가 높은 행동을 선택하여 가치가 높은 것을 선택하는 것이다. 작은 확률이지만 꾸준히 새로운 선택지를 탐색하며 에이전트가 모르는 가치가 높은 무언가를 찾아낼 수 있다. 

선택의 횟수 $T$가 매우 트다면 $ε$이 작아도 모든 선택지를 충분히 탐색할 수 있다. 즉 $T &rightarrow; \infty$ 라면 큰수의 법칙에 따라 모든 선택지의 가치 추정치가 실제 가치로 수렴함을 보장할 수 있다.

$\epsilon$-greedy 알고리즘에서 $ε$는 에이전트가 직접 조절할 수 있다. 보상에 노이즈가 많을 경우 큰$ε$값을 이용하면 최적의 행동을 빨리 찾을 수 있다. 반대로 보상에 노이즈가 업어서 같은 선택을 할 시 같은 보상을 얻는 다면 한번씩만 확인해보면 각 선택의 실제 가치를 알 수 있으므로 $ε$가 0인 $greedy$한 방법이 가장 좋은 결과를 낼 수 있다. 이처럼 상황에 따라 $ε$값의 크기가 영향을 미치므로 각 상황에 알맞은 $ε$를 조절하여야 한다.

강화학습에 과정에서 $ε$를 조절하는 방법도 있다.

### 더 효과적인 방법
