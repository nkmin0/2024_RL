# 2024.03.09

## 머신러닝

머신러닝이란 많은 양의 데이터를 학습한 모델을 바탕으로 입력값이 주어졌을 때 예측, 분류 등을 하는 것이다.

![image](https://github.com/nkmin0/2024_RL/assets/162765658/42cf92ff-3e68-4c60-a12c-db33fc53d23b)


이러한 머신러닝은 크게 3가지로 나눌 수 있다.

### 1.지도학습

지도학습은 컴퓨터가 데이터를 학습하는 과정에서 문제와 정답을 주는 것으로 새로운 문제가 들어왔을 때 이에 대해 분류, 회귀 등을 하는 방법이다.

회귀는 어떠한 y값이 x에 대해 영향을 받는 값이라면 학습을 통해 그래프 등으로 나타내어 학습데이터에 없었던 x값이 들어오더라도 y값을 예측해낼 수 있도록 하는 문제이다.

분류는 라벨링이 되어있는 데이터를 학습한 후 새로운 데이터를 정해진 라벨에 분류하는 문제이다.

### 2.비지도학습

비지도 학습은 지도학습과 반대로 문제에대한 정답을 주지 않고 학습시키는 방법이다.
정답에 한 라벨링이 되어있지 않은 데이터를 통해서 데이터에 대한 패턴 등을 찾는 방법으로 지도학습보다 더 난이도 있는 방법이다.

비지도학습의 종류중 대표적인 것은 클러스터링이다. 클러스터링이란 정답이 없는 데이터를 비슷한 특징을 가진데이터 끼리 군집화 하여 유사한 속성끼리 묶어줌으로써 해결하는 문제이다.

### 3.강화학습

강화학습은 지도학습이나 비지도학습과 달리 정해진 답이 존재하지 않고 자신(agent)이 직접 행동을 통해 환경(environment)을 경험하며 하는 행동(action)에 따라 변하는 상태(state)에 대해 점수, 보상(reward)을 받음으로써 학습하는 방법이다.


## 강화학습

앞서 말했듯이 강화학습은 보상(reward)을 바탕으로 학습하는 것이다. 이러한 보상의 총합을 극대화 함으로서 자신의 목표에 도달해 나간다.

### 순차적 의사 결정

강화학습에서는 보상의 총합을 극대화 하기 위해서 행동의 순서를 학습한다. 예를들어 라면을 먹는다고 할 때 가열을 하고 물을 붓는다든가, 샤워를 할 때 옷을 입고 물기를 닦는것처럼 어떠한 일에는 순서가 필요하다. 의사결정 과정에서 이전선택이 다음 상황에도 영향을 주기 때문에 각 단계들이 논리적으로 연ㄱ결될 수 있도록 해야한다.

### 마르코프 결정 과정

마르코프의 성질이란 "미래는 오로지 현재에 의해 결정된다"라는 것으로 이전 상태가 되는 과정이 미래에 영향을 주지 않는다는 것이다. 

오목, 바둑, 체스같은 게임은 이전 상태가 되는 과정은 영향을 주지 않고 현재의 상황만 보고 다음 수를 결정할수 있다. 이러한 상태를 마르코프한 상태라고 한다. 
마르코프한 상태에는 현재 상태만 보고도 과거의 기록을 알 수 있는 상태이다. 비록 과거의 모든 기록을 볼 수 있는 것은 아니지만 다음 상태를 예측할 때 필요한 중요한 정보는 보존하고 있다.
  
강화학습은 이러한 마르코프한 상태에서 확률이 전이되는 것을 반복하며 진행된다. 마르코프한 상태의 환경에서는 현재 상태와 행동만을 이용해 다음 보상에 대해 예측할 수 있도록 해주어 학습할 때 과거의 기록을 고려하여 학습할 수 있도록 한다.

![image](https://github.com/nkmin0/2024_RL/assets/162765658/ba955ea8-e642-4ca1-9594-b6c96edda107)

에이전트가 환경으로부터 현재상태 S(t)를 받고 에이전트는 S(t)와 자신의 정책(policy)을 바탕으로 어떤 행동을 할지 정한다. 이러한 행동은 환경으로 전달되어 상태를 변화시킨다(--> S(t+1) ). 
이 때 환경에서는 상태와 행동에 따른 보상함수를 계산한 후 반환하여 에이전트에게 전달해 준다.
