# 2024.03.09

## 머신러닝

머신러닝이란 많은 양의 데이터를 학습한 모델을 바탕으로 입력값이 주어졌을 때 예측, 분류 등을 하는 것이다.

![image](https://github.com/nkmin0/2024_RL/assets/162765658/42cf92ff-3e68-4c60-a12c-db33fc53d23b)


이러한 머신러닝은 크게 3가지로 나눌 수 있다.

### 1.지도학습

지도학습은 컴퓨터가 데이터를 학습하는 과정에서 문제와 정답을 주는 것으로 새로운 문제가 들어왔을 때 이에 대해 분류, 회귀 등을 하는 방법이다.

회귀는 어떠한 y값이 x에 대해 영향을 받는 값이라면 학습을 통해 그래프 등으로 나타내어 학습데이터에 없었던 x값이 들어오더라도 y값을 예측해낼 수 있도록 하는 문제이다.

분류는 라벨링이 되어있는 데이터를 학습한 후 새로운 데이터를 정해진 라벨에 분류하는 문제이다.

### 2.비지도학습

비지도 학습은 지도학습과 반대로 문제에대한 정답을 주지 않고 학습시키는 방법이다.
정답에 한 라벨링이 되어있지 않은 데이터를 통해서 데이터에 대한 패턴 등을 찾는 방법으로 지도학습보다 더 난이도 있는 방법이다.

비지도학습의 종류중 대표적인 것은 클러스터링이다. 클러스터링이란 정답이 없는 데이터를 비슷한 특징을 가진데이터 끼리 군집화 하여 유사한 속성끼리 묶어줌으로써 해결하는 문제이다.

### 3.강화학습

강화학습은 지도학습이나 비지도학습과 달리 정해진 답이 존재하지 않고 자신(agent)이 직접 행동을 통해 환경(environment)을 경험하며 하는 행동(action)에 따라 변하는 상태(state)에 대해 점수, 보상(reward)을 받음으로써 학습하는 방법이다.


## 강화학습

앞서 말했듯이 강화학습은 보상(reward)을 바탕으로 학습하는 것이다. 이러한 보상의 총합을 극대화 함으로서 자신의 목표에 도달해 나간다.

### 순차적 의사 결정

강화학습에서는 보상의 총합을 극대화 하기 위해서 행동의 순서를 학습한다. 예를들어 라면을 먹는다고 할 때 가열을 하고 물을 붓는다든가, 샤워를 할 때 옷을 입고 물기를 닦는것처럼 어떠한 일에는 순서가 필요하다. 의사결정 과정에서 이전선택이 다음 상황에도 영향을 주기 때문에 각 단계들이 논리적으로 연결될 수 있도록 해야한다.

### 마르코프 결정 과정

마르코프의 성질이란 "미래는 오로지 현재에 의해 결정된다"라는 것으로 이전 상태가 되는 과정이 미래에 영향을 주지 않는다는 것이다. 

오목, 바둑, 체스같은 게임은 이전 상태가 되는 과정은 영향을 주지 않고 현재의 상황만 보고 다음 수를 결정할수 있다. 이러한 상태를 마르코프한 상태라고 한다. 
마르코프한 상태에는 현재 상태만 보고도 과거의 기록을 알 수 있는 상태이다. 비록 과거의 모든 기록을 볼 수 있는 것은 아니지만 다음 상태를 예측할 때 필요한 중요한 정보는 보존하고 있다.
  
강화학습은 이러한 마르코프한 상태에서 확률이 전이되는 것을 반복하며 진행된다. 마르코프한 상태의 환경에서는 현재 상태와 행동만을 이용해 다음 보상에 대해 예측할 수 있도록 해주어 학습할 때 과거의 기록을 고려하여 학습할 수 있도록 한다.

![image](https://github.com/nkmin0/2024_RL/assets/162765658/ba955ea8-e642-4ca1-9594-b6c96edda107)

에이전트가 환경으로부터 현재상태 S(t)를 받고 에이전트는 S(t)와 자신의 정책(policy)을 바탕으로 어떤 행동을 할지 정한다. 이러한 행동은 환경으로 전달되어 상태를 변화시킨다(--> S(t+1) ). 
이 때 환경에서는 상태와 행동에 따른 보상함수를 계산한 후 반환하여 에이전트에게 전달해 준다.

### 보상함수

강화학습은 상태와 행동에 대한 보상을 받음으로써 학습하는 방법이다. 이러한 보상을 주기 위해 보상함수를 설정해야 한다. 에이전트는 상황에 따라 자신의 보상함수를 설정하여 목표를 이루도록 한다.

![image](https://github.com/nkmin0/2024_RL/assets/162765658/6d07d285-9224-487a-9511-4d0b15481710)

왼쪽 그림의 경우 목표에 도달하면 보상을 주는 간단한 보상함수 이다. 하지만 이러한 경우 목표까지 거리가 멀 때 에이전트가 보상을 받는데 많은 시간이 걸린다는 단점이 있다. 따라서 목표까지 최단경로로 갈 수 있도록 오른쪽 그림과 같이 보상함수를 설정하면 빠르게 에이전트가 보상을 받을 수 있다. 그러나 이 경우에도 최단경로만 고집하게 되어 또다른 변수가 생겼을 때에는 단점이 생길 수 있다.

따라서 상황에 따라 알맞게 보상함수를 설정해야 한다는 것이다. 보상함수를 알맞게 설정하면 빠른 시간 안에 보상을 받을 수 있을 것이고 그렇지 않다면 오랜 시간이 걸릴 것이다.

### 탐색과 활용

![image](https://github.com/nkmin0/2024_RL/assets/162765658/67c0c231-f66b-4745-b32e-e11ff33820ff)

만약 자신이 0에 위치해 있고 3번 이동할 수 있다면 어디로 이동할 것인가? 사람이 보았을 때에는 1,2 를 지나면서 2원 손해를 보더라도 3에 도달하여 10원을 얻으면 결과적으로 8원 이득으로 최고의 결과라는 것을 알 수 있다. 하지만 -3, -2, 2, 3에 위치에서 얼마나 돈을 얻을지, 잃을지 모른다면 위와 같은 선택을 하기 쉽지 않다.

-1, 0, 1에 대한 정보만 있고 다른 위치에 대한 정보를 가지고 있지 않는다고 하자. 그렇다면 자신이 아는 정보만을 활용하면 왼쪽, 오른쪽, 왼쪽을 선택하여 2원을 얻는것이 최선이 될것이다. 최대 10원이라는 결과를 얻을 수 있지만 3에서 10원을 준다는 정보를 가지고 있지 않아 할 수 없는것이다.

따라서 강화학습에서는 새로운 정보에 대한 탐색과 알고있는 정보에 대한 활용이 중요하게 작용한다는 것이다.

---

## MountainCar

![image](https://github.com/nkmin0/2024_RL/assets/162765658/e32a74aa-8b6c-4c55-968d-934ee906869a)

언덕 아래에 있는 수례를 언덕 위 깃발에 도달시켜야 한다. 

- 왼쪽으로 가는 페달을 밟는다.
- 가만히 있는다.
- 오른쪽으로 가는 패달을 밟는다.

단순히 오른쪽으로만 갔을 때에는 속력이 부족하여 언덕을 올라가지 못한다.
위와같은 행동을 할 수 있을 때 깃발에 도달시켜야 하는 방법을 찾아야 한다.

### 정책 정의하기

```python
class RandomPolicy():
  # 클래스 선언
    def __init__(self, action_space):
      # 행동공간에 대한 정보 저장
        self.action_space = action_space

    # 클래스를 호출할 수 있도록 해주는 함수
    def __call__(self, state):
      # 상태를 받았을 때
      # 랜덤하게 action을 하도록
        return self.action_space.sample()
```

강화학습에서 정책은 환경이 변화할 때 인공지능이 어떻게 행동할 지를 결정하는 것으로 이 코드를 수정해서 목표에 쉽게 도달할 수 있도록 한다.

```python
class MyPolicy():
  def __init__(self, action_space):
    self.action_space = action_space

  def __call__(self, state):
    x, v = state
    next_action = 2

    if v>0:
      next_action = 2
    elif v<0:
      next_action = 0
    else:
      next_action = 1

    return next_action
```

위는 내가 MountainCar 문제를 해결하기 위해 짠 정책이다. 속도와 같은방향으로 페달을 밟도록 하여 좌우를 이동하면서 가속을 받아 깃발에 도달할 수 있도록 하였다.

---

## BlackJack

![image](https://github.com/nkmin0/2024_RL/assets/162765658/e7d7c0ca-fe38-41a7-95f1-84ba0d41229c)

블랙잭은 자신의 카드의 합이 21을 넘지 않으면서 21에 더 가까운 사람이 이기는 게임이다.

- Jack, Queen, King 카드는 모두 10으로 계산한다.
- Ace는 1 또는 11중 원하는 숫자로 계산할 수 있다.
- 나머지 숫자 카드(2~9)는 그 숫자 그대로 계산한다.

블랙잭에서 카드에는 위와같은 규칙이 적용된다.

이때 내가 할 수 있는 행동은 두가지가 있다.

- Hit
  Hit를 선택할 시 자신의 카드를 한장 추가한다. 만약 자신의 카드의 합이 21이 넘어가면 패배(Bust)한다.
- Stick
  Stick을 선택할 시 딜러에 차례로 넘어간다.

딜러는 무조건 아래의 규칙을 따른다.

- 게임을 시작할 때 딜러는 카드 두장을 각각 앞면, 뒷면으로 놓는다.
- 모든 플레이어의 플레이가 끝나면(Stick) 뒷면으로 덮어둔 카드를 오픈한다.
- 만약 딜러의 카드의 총합이 17 이상이면 멈추고 16 이하이면 카드를 더 받는다.
- 만약 딜러의 카드가 21이 넘어가면(Bust) 21이 넘지 않은 플레이어는 이긴다.
- 딜러의 카드가 21이 넘어가지 않으면(not Bust) 21에 더 가까운 사람이 이기게 된다.

블랙잭에서 상태는 (자신의 카드의 총합, 딜러의 오픈된 카드, 자신의 사용가능한 Ace의 존재여부)로 들어온다. 블랙잭에서 이기기 위해서는 자신의 카드의 총합이 N일 때 Hit를 하는것이 이득인지 Stick을 하는것이 이득인지를 아는것이 중요하다 생각한다.

```python
p=[[1, 2] for _ in range(23)]

class MyPolicy():
  def __init__(self, action_space):
    self.action_space = action_space
    #self.p = [[1, 1] for _ in range(23)]
  def __call__(self, state):
    cur, D, a = state
    q=random.random()
    if cur>=21:
      return 0
    if q < p[cur][0] / p[cur][1]:
      return 1
    else:
      return 0
```

따라서 위와 같이 확률 배열 p를 선언하여 p의 확률로 Hit를 하고 1-p의 확률로 Stick을 하도록 한다.

```python
env = gym.make('Blackjack-v1')

state, _ = env.reset()

win=0
draw=0
lose=2

bust=1
low=1
```

```python
for i in range(100000):

  done = False
  reward = 0
  while not done:
      state, _ = env.reset()

      agent = MyPolicy(env.action_space)
      action = agent(state)

      pre_state = state
      pre_my, _, __ = pre_state
      state, reward, done, _, info = env.step(action)

      if action==1:
        p[pre_my][1]+=hit
        if reward!=-1:
          p[pre_my][0]+=hit

      elif action==0 and pre_my<=21:
        if reward!=0:
          p[pre_my][1]+=stop
          if reward==-1:
            p[pre_my][0]+=stop
          elif reward==1 and p[pre_my][0]>1:
            p[pre_my][0]-=stop



  if reward == 1.0:
    win+=1
  elif reward == -1.0:
    lose+=1
    bust+=action
    low=lose-bust
  else:
    draw+=1

  hit = low/(bust+low)
  stop = bust/(bust+low)

lose-=2
bust-=1
low-=1

print("win : ", round(win/(win+draw+lose) *100,2), "%",sep='')
print("draw : ", round(draw/(win+draw+lose) *100,2), "%",sep='')
print("lose : ", round(lose/(win+draw+lose) *100,2), "%",sep='')
print()
print("bust : ", bust)
print("low : ", low)

env.close()
```

현재 상태가 N일 때 Hit를 했는데 21을 넘지 않으면(not Bust) p을 올려준다.

반대로 Stick을 하였을 때 승리로 게임이 끝나면 p를 낮추고(1-p를 올리고) 패배로 게임이 끝나면 Stick을 하는 확률을 낮추기 위해 P를 올려준다.

위 방법을 통해 약 39%의 승률이 나왔다.
